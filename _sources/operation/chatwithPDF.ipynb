{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aaVT4EPYWvj"
   },
   "source": [
    "# OpenAI API & LangChain Chat with PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlBpkM6IcFDO"
   },
   "source": [
    "In this tutorial I'd like to introduce you to the API to the LLMs, and in the end, we will build something useful for ourselves, a conversation agent that we can chat with about a certain paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1UDXPr4XGgz"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRJbKVvrXE54"
   },
   "source": [
    "### `OpenAI_API_KEY`\n",
    "We also need the API key from Openai to get access to their models.\n",
    "\n",
    "https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install pypdf\n",
    "%pip install chromadb\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzdVllllS1EX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Openai API\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYlysKZrW_FH"
   },
   "source": [
    "### Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez1emacjQlEY"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import textwrap\n",
    "from os.path import join\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8LAq9ShcxRY"
   },
   "source": [
    "## OpenAI LLM APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tslmnCJtkxQK"
   },
   "source": [
    "Useful links for checking price and cost\n",
    "* https://openai.com/pricing\n",
    "* https://platform.openai.com/account/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBddymkoeHOh"
   },
   "source": [
    "Sample text from https://arxiv.org/abs/2304.02643\n",
    "> We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZt1ZKaheEh2"
   },
   "outputs": [],
   "source": [
    "text = \"\"\" We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXlFutwUU7Ul"
   },
   "source": [
    "### OpenAI text embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaMqDUgtYCnx"
   },
   "source": [
    "OpenAI provides API to embed chunk of texts into a single vector. This vector embedding can be used to retrieve these text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJT4UyuwVAzd"
   },
   "outputs": [],
   "source": [
    "response = openai.Embedding.create(\n",
    "  model=\"text-embedding-ada-002\",\n",
    "  input=text\n",
    ")\n",
    "\n",
    "embeddings = response['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgTIqj05Y6Rw"
   },
   "source": [
    "### OpenAI text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyEXH-ucc2QE"
   },
   "source": [
    "This is the API for GPT3 models, so we can send prompts to them and let it complete the text. Note that, the task is just to complete the text, so it may not answer your question. To really do so, **Prompt Engineering** is important.\n",
    "\n",
    "See official examples in https://platform.openai.com/examples/default-chat\n",
    "\n",
    "Generally, for these prompts\n",
    "\n",
    "* Prepending *system messages* before the question will condition the GPT model and make the answer more helpful.  \n",
    "* Adding one or a few examples of what type of output do you expect and what format that is will also help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9Xd8QxAY_2E"
   },
   "outputs": [],
   "source": [
    "prompt = \"I'd like to continue generating a paper's introduction paragraph based on the abstract. \\n\\nAbstract\\n\"+text+\"\\nMain Text\\nIntroduction\\n\"\n",
    "\n",
    "complete_resp = openai.Completion.create(\n",
    "  # model=\"text-davinci-003\", # more powerful and expensive\n",
    "  model=\"text-curie:001\", # cheaper\n",
    "  prompt=prompt,\n",
    "  max_tokens=256,\n",
    "  temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtNzn4QUdM5c"
   },
   "source": [
    "Prompt it to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUY-Mos3dNPk"
   },
   "outputs": [],
   "source": [
    "pre_prompt = \"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \\\"Unknown\\\".\\n\\nQ: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: Unknown\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: How many squigs are in a bonk?\\nA: Unknown\\n\\n\"\n",
    "question = \"Q: When is the Eiffel Tower built?\"\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=pre_prompt + question + \"\\nA:\",\n",
    "  temperature=0,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgQ-dE-Nf4td"
   },
   "source": [
    "Prompt it to chat with you as friendly AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbsWCJ_Zf8oJ"
   },
   "outputs": [],
   "source": [
    "pre_prompt = \"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\\n\\n\"\n",
    "prompt = \"\"\"\n",
    "Human: Hello, who are you?\n",
    "AI: I am an AI created by OpenAI. How can I help you today?\n",
    "Human: I'd like to cancel my subscription.\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=pre_prompt + prompt + \"\\nA:\",\n",
    "  temperature=0,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hHbuKdmgkJL"
   },
   "source": [
    "### OpenAI ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hWV0BEfhjtt"
   },
   "source": [
    "This is the API for ChatGPT, and we can more directly ask questions or have a conversation with it without much prompt tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21MB7vZFgmsK"
   },
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you tell me a really funny joke to light my day up?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-eGdEmYXeLJ"
   },
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlkkyE2Ci9Ku"
   },
   "source": [
    "The idea is like this\n",
    "1. First, divide the paper / doc into many chunks.\n",
    "2. Embed each chunk into a vector via OpenAI GPT embedding\n",
    "3. Store these vectors in a database (`chromedb`)\n",
    "4. When a question comes, embed it and retrieve the most similar text chunks.\n",
    "5. Send these chunks and the questions to ChatGPT API, and let it answer the question.\n",
    "\n",
    "This is a complex pipeline, so `langchain` works as the glue that connect every parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1i2p9PgzZvr"
   },
   "source": [
    "![Chat with PDF](https://raw.githubusercontent.com/Animadversio/TransformerFromScratch/main/media/ChatPDF_Schematics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lj-BgrgWWmN"
   },
   "source": [
    "**Required libraries**\n",
    "\n",
    "* `openai` provides API of openai GPT models\n",
    "* `langchain` a recent framework that chain together the GPT models to build applications.\n",
    "* `pypdf` pdf parser in python.\n",
    "* `chromadb` a vector database that manage the embedding vectors and handle the info retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P8G9z12Ujqp"
   },
   "source": [
    "### Tool Chaining with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A44S7rLGibiS"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader # for loading the pdf\n",
    "from langchain.embeddings import OpenAIEmbeddings # for creating embeddings\n",
    "from langchain.vectorstores import Chroma # for the vectorization part\n",
    "from langchain.chains import ChatVectorDBChain # for chatting with the pdf\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI # the LLM model we'll use (CHatGPT)\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Plmqyemw3SQy"
   },
   "source": [
    "### Parse and Split the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_LeVUAoUVML"
   },
   "source": [
    "Download the pdf and parse it through python PDF reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrwP-AP0St8T"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_pdf(arxiv_id, save_root=\"\"):\n",
    "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    r = requests.get(url, allow_redirects=True,)\n",
    "    open(join(save_root, f\"{arxiv_id}.pdf\"), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaQQF7i5S5xf"
   },
   "outputs": [],
   "source": [
    "arxiv_id = \"1706.03762\"#\"2304.02643\"\n",
    "# This is the Segment Everything paper from Meta https://arxiv.org/abs/2304.02643,\n",
    "# change to sth you are curious about!\n",
    "\n",
    "download_pdf(arxiv_id)\n",
    "pdf_path = f\"{arxiv_id}.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE5ijphBTNuT"
   },
   "source": [
    "Let's look at what lies within the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7lbaJvaTOyC",
    "outputId": "99d0fed1-2480-4d75-ccac-436d39b92593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '1706.03762.pdf', 'page': 0}\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani\u0003\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer\u0003\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar\u0003\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit\u0003\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones\u0003\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez\u0003y\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser\u0003\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin\u0003z\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\n",
      "in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\n",
      "\u0003Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "yWork performed while at Google Brain.\n",
      "zWork performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].metadata)\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57CD6D363WA0"
   },
   "source": [
    "For other document types that are loadable via `langchain`, see https://python.langchain.com/en/latest/modules/indexes/document_loaders.html\n",
    "\n",
    "Including\n",
    "* Notion\n",
    "* Markdown\n",
    "* Word\n",
    "* PowerPoint\n",
    "* epubs\n",
    "* WhatsAPP\n",
    "* Telegram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OX8v4XD2HJY"
   },
   "source": [
    "### Embedding Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2pbfGgaTHAj"
   },
   "source": [
    "Create embedding (not computed yet, just a place holder), and then save them in a database.\n",
    "\n",
    "`Chroma` is the database lib to collect the embeddings and retrieve content with vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rki6_xe1TD6g",
    "outputId": "5105de79-04a9-4ec1-ddab-d6bed11dc2bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: 1706.03762\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings() # GPT3-Ada-Embedding\n",
    "vectordb = Chroma.from_documents(pages, embedding=embeddings,\n",
    "              persist_directory=pdf_path.replace(\".pdf\", \"\"), )\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_km96QOp2KIl"
   },
   "source": [
    "### Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21fLSQTgUIE0"
   },
   "source": [
    "Create a conversation retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Rycjxq9TdE7"
   },
   "outputs": [],
   "source": [
    "pdf_qa = ConversationalRetrievalChain.from_llm(\n",
    "      ChatOpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo\"), # ChatGPT API\n",
    "              vectordb.as_retriever(),\n",
    "              return_source_documents=True,\n",
    "              max_tokens_limit=4097)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yD4cKQdUCe0"
   },
   "source": [
    "### Saving function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf_CJHi5WHLe"
   },
   "source": [
    "This is the utility function to append new questions and answers to a `md` and `pkl` file. So it's easier to look at chat histroy post hoc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Idiap_nKTr_5"
   },
   "outputs": [],
   "source": [
    "def save_qa_history(query, result, qa_path,):\n",
    "    uid = 0\n",
    "    while os.path.exists(join(qa_path, f\"QA{uid:05d}.pkl\")):\n",
    "        uid += 1\n",
    "    pkl.dump((query, result), open(join(qa_path, f\"QA{uid:05d}.pkl\"), \"wb\"))\n",
    "\n",
    "    pkl_path = join(qa_path, \"chat_history.pkl\")\n",
    "    if os.path.exists(pkl_path):\n",
    "        chat_history = pkl.load(open(pkl_path, \"rb\"))\n",
    "    else:\n",
    "        chat_history = []\n",
    "    chat_history.append((query, result))\n",
    "    pkl.dump(chat_history, open(pkl_path, \"wb\"))\n",
    "\n",
    "    # print to a markdown file with formatting for reading\n",
    "    with open(os.path.join(qa_path, \"QA.md\"), \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n**Question:**\\n\\n\")\n",
    "        f.write(query)\n",
    "        f.write(\"\\n\\n**Answer:**\\n\\n\")\n",
    "        f.write(result[\"answer\"])\n",
    "        f.write(\"\\n\\nReferences:\\n\\n\")\n",
    "        for doc in result[\"source_documents\"]:\n",
    "            f.write(\"> \")\n",
    "            f.write(doc.page_content[:250])\n",
    "            f.write(\"\\n\\n\")\n",
    "        f.write(\"-------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3wSNQsvT8jU"
   },
   "source": [
    "### Demo: Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79puHBnWT7KI"
   },
   "outputs": [],
   "source": [
    "qa_path = pdf_path.replace(\".pdf\", \"\") + \"_qa_history\"\n",
    "os.makedirs(qa_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAnLPiw6TfVS",
    "outputId": "e5e2b026-72d4-447d-9619-2062caaf1756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "While the Transformer has shown to be effective in various natural language\n",
      "processing tasks, it still has some limitations. One of the main limitations is\n",
      "that it requires significant computational resources for training, which can\n",
      "make it challenging to train on large datasets. Additionally, it may not perform\n",
      "as well as recurrent models on tasks that require modeling of long-term\n",
      "dependencies in the input sequence. Finally, the attention mechanism used in the\n",
      "Transformer can be sensitive to the order of the input sequence, which can be a\n",
      "problem in tasks where the order of the input is important.\n",
      "\n",
      "References\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b\n",
      "\n",
      "\n",
      "constraints and is signiﬁcantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models \n",
      "\n",
      "\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "\n",
      "\n",
      "transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\n",
      "efforts \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the limitations of this architecture?\"\n",
    "# query = \"How is this method related to previous segmentation methods e.g. Mask RCNN\"\n",
    "# More questions?\n",
    "\n",
    "result = pdf_qa({\"question\": query, \"chat_history\": \"\"})\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(textwrap.fill(result[\"answer\"], 80))\n",
    "print(\"\\nReferences\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.page_content[:100])\n",
    "    print(\"\\n\")\n",
    "\n",
    "save_qa_history(query, result, qa_path,)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
