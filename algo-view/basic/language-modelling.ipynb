{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vePKj2Q6Huai"
      },
      "source": [
        "# Transformers for Language Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5MqS6YYVoUr"
      },
      "source": [
        "As we have familiarized ourselves with the attention mechanism in Transformers, now let's see how these models can be trained as language models.\n",
        "\n",
        "* In the first part, I prepared a few pretrained language models for you to examine and play with.\n",
        "* In the second part, we will train a transformer language model from scratch.\n",
        "  * We will handcraft a simple English language with [Probabilistic Context free Grammar](https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar).\n",
        "  * Then let the GPT learn this artificial language, and we will examine the emergent phenomena from training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLMOx_hoJCJM"
      },
      "source": [
        "## Play with Pre-trained transformers language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQBE1UBKJNUK"
      },
      "source": [
        "`transformers` library from huggingface is the *de facto* standard library for loading and using transformer architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOAOnHgbNgbQ"
      },
      "source": [
        "Let's load some common models and see their inner architecture. Specifically, we will look at two classic models, BERT and GPT2. (developped ~ 2018)\n",
        "\n",
        "As we mentioned in the lecture, they are both transformers but with some prominal differences\n",
        "\n",
        "* BERT: Bidirectional Encoder Representations from Transformers\n",
        "  * **Architecture**: All-to-all attention. Transformer Encoder.\n",
        "  * **Objective**: Masked language modelling\n",
        "  * **Usage**:\n",
        "    * Can fill in blanks and perform cloze task\n",
        "    * Provide representation for many downstream language understanding tasks.\n",
        "* GPT2: Generative Pre-trained Transformer 2\n",
        "  * **Architecture**: Causal attention. Transformer Decoder\n",
        "  * **Objective**: autoregressive language modelling\n",
        "  * **Usage**:\n",
        "    * Can continue prompt and answer questions\n",
        "    * Can be finetuned to follow task specific instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O5wkUxciWes"
      },
      "source": [
        "Here are the conceptual picture of BERT and GPT2, let's keep it in mind and try to map them to the modules we see\n",
        "\n",
        "**BERT**\n",
        "\n",
        "![BERT (Transformer encoder)](https://iq.opengenus.org/content/images/2020/06/encoder-1.png)\n",
        "\n",
        "**GPT2**\n",
        "\n",
        "![](https://miro.medium.com/v2/1*jbcwhhB8PEpJRk781rML_g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9qWUlFcfhNq"
      },
      "source": [
        "### Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0SBgGkDf5Qa"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from transformers import pipeline, BertModel, BertConfig, BertTokenizer, BertLMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbBL4AOEOCxg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "def recursive_print(module, prefix=\"\", depth=0, deepest=3):\n",
        "    \"\"\"Simulating print(module) for torch.nn.Modules\n",
        "        but with depth control. Print to the `deepest` level. `deepest=0` means no print\n",
        "    \"\"\"\n",
        "    if depth == 0:\n",
        "        print(f\"[{type(module).__name__}]\")\n",
        "    if depth >= deepest:\n",
        "        return\n",
        "    for name, child in module.named_children():\n",
        "        if len([*child.named_children()]) == 0:\n",
        "            print(f\"{prefix}({name}): {child}\")\n",
        "        else:\n",
        "            if isinstance(child, nn.ModuleList):\n",
        "                print(f\"{prefix}({name}): {type(child).__name__} len={len(child)}\")\n",
        "            else:\n",
        "                print(f\"{prefix}({name}): {type(child).__name__}\")\n",
        "        recursive_print(child, prefix + \"  \", depth + 1, deepest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-Oe_jUNp5A"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktwsjxaYjoSh",
        "outputId": "cf594f06-fb52-4616-bff3-145540d48bc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Accessing the model configuration\n",
        "BERTtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "BERTmodel = BertModel.from_pretrained('bert-base-uncased')\n",
        "BERTconfig = BERTmodel.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdzl86yCj9Pq",
        "outputId": "f791234e-4d19-4cfb-fdd2-00497323a870"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"_name_or_path\": \"bert-base-uncased\",\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.26.1\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BERTconfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFbIGvsCTZuw"
      },
      "source": [
        "### Let BERT do a cloze test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4xA_U0eTGHI"
      },
      "outputs": [],
      "source": [
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker(\"Hello I'm a [MASK] model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U0O8Eu8VHaY"
      },
      "source": [
        "### Inner structure of BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4BEywfcNzkQ"
      },
      "outputs": [],
      "source": [
        "recursive_print(BERTmodel,deepest=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPBgblJVOQYA"
      },
      "outputs": [],
      "source": [
        "recursive_print(BERTmodel.encoder,deepest=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSbcC8x_OUbY"
      },
      "outputs": [],
      "source": [
        "recursive_print(BERTmodel.encoder.layer[0],deepest=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gv3oCVPcOago"
      },
      "outputs": [],
      "source": [
        "recursive_print(BERTmodel.encoder.layer[0].attention,deepest=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb8DCNByNoKK"
      },
      "source": [
        "### GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjP1TOErh1_J"
      },
      "outputs": [],
      "source": [
        "GPTtokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "GPTmodel  = GPT2Model.from_pretrained('gpt2')\n",
        "GPTconfig = GPT2Config.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV5E8ui1j_yx",
        "outputId": "fb0d212a-7483-4a56-b95b-ac9d86182517"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"architectures\": [\n",
              "    \"GPT2LMHeadModel\"\n",
              "  ],\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_ctx\": 1024,\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 12,\n",
              "  \"n_positions\": 1024,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"task_specific_params\": {\n",
              "    \"text-generation\": {\n",
              "      \"do_sample\": true,\n",
              "      \"max_length\": 50\n",
              "    }\n",
              "  },\n",
              "  \"transformers_version\": \"4.26.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50257\n",
              "}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GPTconfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F6T5YeKUlyZ"
      },
      "source": [
        "### Let GPT say something"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtrklIIoTg_R"
      },
      "outputs": [],
      "source": [
        "GPTLMmodel = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP1uqpjEUACy"
      },
      "outputs": [],
      "source": [
        "input_ids = GPTtokenizer.encode(\"The meaning of life\")\n",
        "out_ids = GPTLMmodel.generate(torch.tensor(input_ids).long()[None,:],\n",
        "              max_length=32, do_sample=True, top_k=0, top_p=0.9, temperature=0.7,\n",
        "              num_return_sequences=5,)\n",
        "for seq in out_ids:\n",
        "  print(GPTtokenizer.decode(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqbaXyKqUfyJ"
      },
      "source": [
        "### Inner structure of GPTmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhWwubtXOyRp"
      },
      "outputs": [],
      "source": [
        "recursive_print(GPTmodel,deepest=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t79MsKGJO0w4"
      },
      "outputs": [],
      "source": [
        "recursive_print(GPTmodel.h,deepest=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEgFEVV5O5IA"
      },
      "outputs": [],
      "source": [
        "recursive_print(GPTmodel.h[0],deepest=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0-aY8vhO7l4"
      },
      "outputs": [],
      "source": [
        "recursive_print(GPTmodel.h[0].attn,deepest=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPcIcXjuO-LY"
      },
      "outputs": [],
      "source": [
        "recursive_print(GPTmodel.h[0].mlp,deepest=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PArXwg9GJIqJ"
      },
      "source": [
        "## Train a transformer to \"speak your own language\"!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4St1-R-JOvB"
      },
      "source": [
        "So let's design an artificial language with simple grammar and let the transformer learn to \"speak\" it. More technically, this means, we hand-craft a probabilistic model of language (sequence) and then let the transformer learn it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B3jXbbn8OQh"
      },
      "source": [
        "### Probabilistic Generative Grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoeXKEeg8u5J"
      },
      "source": [
        "First let's make our *simplified English grammar*.\n",
        "\n",
        "By building a set of probabilistic generative grammar, or more technically, [Probabilistic context-free grammar, (PCFG)](https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar)\n",
        "\n",
        "This language has these part of speeches\n",
        "* Noun\n",
        "* Intransitive verb\n",
        "* Transitive verb\n",
        "* Article\n",
        "* Adjective\n",
        "* Conjunction \"that\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8yLtchyJPTh"
      },
      "outputs": [],
      "source": [
        "#%% list of words and their parts of speech\n",
        "# nouns\n",
        "noun_list = [\"cat\", \"dog\", \"fox\", \"bird\", \"horse\", \"sheep\", \"cow\", \"bear\", \"zebra\", \"giraffe\"]\n",
        "# intransitive verb\n",
        "intrans_verb_list = [\"ran\", \"jumped\", \"swam\", \"flew\", \"walked\", \"slept\", \"sat\", \"stood\", \"danced\"]\n",
        "# transitive verbs that took an object or a clause\n",
        "trans_verbs_list = [\"saw\", \"heard\", \"smelled\", ]\n",
        "# adjectives\n",
        "adj_list = [\"big\", \"small\", \"tall\", \"short\", \"long\", \"wide\", \"fat\", \"thin\", \"round\", \"square\", \"smart\", \"pretty\"]\n",
        "# adverbs\n",
        "article_list = [\"the\", \"a\"]\n",
        "# conjunctive that introduces a clause.\n",
        "conjunction_list = [\"that\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-O7PkDO9Kmo"
      },
      "source": [
        "This language also has the following grammar, i.e. a set of substitution rules. For example,\n",
        "\n",
        "$$S\\to NP,VP$$\n",
        "means a sentence can be substituted by a Noun phrase (NP) + a Verb phrase (VP).\n",
        "$$VP\\to TV, Conj, NP, VP$$\n",
        "means a verb phrase can be substituted by a transitive verb (TV) + a conjunction (Conj) + a Noun phrase (NP) + a Verb phrase (VP).\n",
        "\n",
        "The full set of rules are following, feel free to modify it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T591chDh9Xhw"
      },
      "outputs": [],
      "source": [
        "# Rules for mapping part of speech to words\n",
        "word_map = {\n",
        "    \"N\": noun_list,\n",
        "    \"IV\": intrans_verb_list,\n",
        "    \"TV\": trans_verbs_list,\n",
        "    \"Adj\": adj_list,\n",
        "    \"Article\": article_list,\n",
        "    \"Conj\": conjunction_list,\n",
        "}\n",
        "# Grammar for generating sentences\n",
        "rules = {\n",
        "    # sentence\n",
        "    \"S\": [[\"NP\", \"VP\"]],\n",
        "    # noun phrase\n",
        "    \"NP\": [[\"Article\", \"N\"],\n",
        "           [\"Article\", \"Adj\", \"N\"],\n",
        "           [\"Article\", \"Adj\", \"Adj\", \"N\"],\n",
        "           [\"Article\", \"N\", \"Conj\", \"IV\"]],\n",
        "    # verb phrase\n",
        "    \"VP\": [[\"IV\"],\n",
        "           [\"TV\", \"NP\"],\n",
        "           [\"TV\", \"Conj\", \"NP\", \"VP\"], ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgB8xXYb_FzY"
      },
      "source": [
        "### Sample from probabilistic generative grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqcnR1tO-LUp"
      },
      "source": [
        "Now, see what these rules can generate using **probabilistic generative grammar**, i.e. recursively applying the substitution rules to an initial token $S$ to elaborate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESX43r02-eGx",
        "outputId": "ab72c614-8376-4c94-8a65-c758a819842f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['NP', 'VP']\n",
            "['Article', 'N', 'Conj', 'IV', 'TV', 'Conj', 'NP', 'VP']\n",
            "['Article', 'N', 'Conj', 'IV', 'TV', 'Conj', 'Article', 'Adj', 'N', 'IV']\n",
            "['Article', 'N', 'Conj', 'IV', 'TV', 'Conj', 'Article', 'Adj', 'N', 'IV']\n",
            "the dog that swam smelled that the round sheep slept\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "def generate_sentences(rules, word_map, max_depth=3, show=False):\n",
        "    \"\"\" A sentence generator with probabilistic generative grammar. \"\"\"\n",
        "    initial_token = \"S\"\n",
        "    sentence = [initial_token]\n",
        "    depth = 0\n",
        "    while True:\n",
        "        next_sentence = []\n",
        "        fully_expanded = True\n",
        "        for token in sentence:\n",
        "            if token in rules:\n",
        "                # expand the phrase\n",
        "                if depth < max_depth:\n",
        "                    next_sentence.extend(random.choice(rules[token]))\n",
        "                else:\n",
        "                    # to limit complexity, we stop adding clauses\n",
        "                    next_sentence.extend(random.choice(rules[token][:-1]))  # don't expand into the last conjunctive rule\n",
        "\n",
        "                fully_expanded = False\n",
        "            else:\n",
        "                next_sentence.append(token)\n",
        "\n",
        "        sentence = next_sentence\n",
        "        depth += 1\n",
        "        if show:\n",
        "            print(sentence)\n",
        "        if fully_expanded:\n",
        "            break\n",
        "    # turn tokens into words\n",
        "    verbal_sentence = []\n",
        "    for token in sentence:\n",
        "        word = random.choice(word_map[token])\n",
        "        verbal_sentence.append(word)\n",
        "    sent_str = \" \".join(verbal_sentence)\n",
        "    return verbal_sentence, sent_str\n",
        "\n",
        "\n",
        "word_seq, sentence_str = generate_sentences(rules, word_map, show=True, max_depth=5)\n",
        "print(sentence_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcTCYXJB-rlg"
      },
      "source": [
        "Surely you can generate all kinds of nonsensical sentences which are grammatically correct!\n",
        "> a small big cow smelled the thin fat zebra\n",
        "\n",
        "But let's ignore it for a moment and let a transformer learn to \"speak\" this simply English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb_liucABJnA"
      },
      "source": [
        "### Tokenization: map words to numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMyAT6ekBN8Y"
      },
      "source": [
        "For a computational system to understand language, first, we need to break the sentence into words and turn the words into something they know , indices. This is known as tokenization and encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdEB1j1MBe9a"
      },
      "outputs": [],
      "source": [
        "full_word_set = set(sum([words for words in word_map.values()], []))\n",
        "dictionary = {word: i for i, word in enumerate(full_word_set)}  # Mapping words to indices\n",
        "dictionary[\"[End]\"] = len(dictionary)\n",
        "EOS_ID = dictionary[\"[End]\"]  # end of sentence token\n",
        "PAD_ID = len(dictionary)  # padding token\n",
        "dictionary[\"\"] = PAD_ID\n",
        "\n",
        "inverse_dictionary = {i: word for word, i in dictionary.items()}   # Mapping indices to words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLawdrkoBnpo"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentence(sentence):\n",
        "    \"\"\" Tokenize a sentence into a list of words. \"\"\"\n",
        "    word_seq = sentence.split(\" \")\n",
        "    return word_seq\n",
        "\n",
        "\n",
        "def encode_sentence(sentence):\n",
        "    \"\"\" Encode a sentence into a list of indices. \"\"\"\n",
        "    word_seq = tokenize_sentence(sentence)\n",
        "    inds = encode2indices(word_seq)\n",
        "    return inds\n",
        "\n",
        "\n",
        "def encode2indices(word_seq):\n",
        "    \"\"\" Encode a list of words into a list of indices. \"\"\"\n",
        "    inds = [dictionary[word] for word in word_seq]\n",
        "    return inds\n",
        "\n",
        "\n",
        "def decode2words(indices):\n",
        "    \"\"\" Decode a list of indices into a list of words. \"\"\"\n",
        "    words = [inverse_dictionary[ind] for ind in indices]\n",
        "    return words\n",
        "\n",
        "\n",
        "def decode2sentence(indices):\n",
        "    \"\"\" Decode a list of indices into a sentence. \"\"\"\n",
        "    words = decode2words(indices)\n",
        "    sentence = \" \".join(words)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOONetGNBsUg",
        "outputId": "9fd25b6c-3350-4137-9846-98db06741c05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[12, 19, 8, 27, 12, 14, 32, 1]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encode_sentence(\"a smart fox saw a cat that ran\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv1r6svjCNbw"
      },
      "source": [
        "### Build Sequence Dataset: `pad_seq` etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCEr77XUCXPA"
      },
      "source": [
        "Since we don't have a fixed dataset, but a generative model of language, we can simply generate new sentences on the fly and fed it to our transformer. (this is usually not the case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjreQ9A1CWuQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def batch_sampler(batch_size=32, max_len=128):\n",
        "    batch = []\n",
        "    for i in range(batch_size):\n",
        "        word_seq, _ = generate_sentences(rules, word_map)\n",
        "        word_seq.append(\"[End]\")  # add this marker to say, the sentence has ended.\n",
        "        inds = encode2indices(word_seq)\n",
        "        batch.append(torch.tensor(inds, dtype=torch.long))\n",
        "    # pad the batch to be equal length, same len as the longest senetence.\n",
        "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=PAD_ID)\n",
        "    # chuck to the max_len\n",
        "    padded_batch = padded_batch[:, :max_len]\n",
        "    return padded_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdutjfrLC6YZ"
      },
      "outputs": [],
      "source": [
        "batch = batch_sampler()\n",
        "print(batch.shape)\n",
        "for j in range(batch.shape[0]):\n",
        "    print(decode2sentence(batch[j].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mY8pNhGAEa4"
      },
      "source": [
        "### Train a transformer on this language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ErkXcuHAHQp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from transformers import GPT2Model, GPT2Tokenizer, GPT2LMHeadModel, GPT2Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtgQC-vgAYcw"
      },
      "outputs": [],
      "source": [
        "miniGPTconfig = GPT2Config(vocab_size=len(dictionary), n_positions=128, n_ctx=128,\n",
        "                           n_embd=64, n_layer=3, n_head=8,\n",
        "                           eos_token_id=EOS_ID, pad_token_id=PAD_ID)\n",
        "miniGPT = GPT2LMHeadModel(miniGPTconfig, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Libcy4nWCChr"
      },
      "source": [
        "Finally, let's run our training loops!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtyKfvjzB9IQ",
        "outputId": "b5d0eab5-09ad-4d6a-8f93-66c6fca2bd5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, batch 0, loss 3.277460813522339\n",
            "epoch 0, batch 1, loss 2.996049642562866\n",
            "epoch 0, batch 2, loss 2.8948705196380615\n",
            "epoch 0, batch 3, loss 2.891660451889038\n",
            "epoch 0, batch 4, loss 2.784578323364258\n",
            "epoch 0, batch 5, loss 2.746859312057495\n",
            "epoch 0, batch 6, loss 2.7404227256774902\n",
            "epoch 0, batch 7, loss 2.700308084487915\n",
            "epoch 0, batch 8, loss 2.6572651863098145\n",
            "epoch 0, batch 9, loss 2.591931104660034\n",
            "epoch 0, batch 10, loss 2.575809955596924\n",
            "epoch 0, batch 11, loss 2.546907901763916\n",
            "epoch 0, batch 12, loss 2.494894504547119\n",
            "epoch 0, batch 13, loss 2.4887619018554688\n",
            "epoch 0, batch 14, loss 2.452965021133423\n",
            "epoch 0, batch 15, loss 2.3869802951812744\n",
            "epoch 0, batch 16, loss 2.369704008102417\n",
            "epoch 0, batch 17, loss 2.3279871940612793\n",
            "epoch 0, batch 18, loss 2.3047902584075928\n",
            "epoch 0, batch 19, loss 2.260281801223755\n",
            "epoch 0, batch 20, loss 2.2382638454437256\n",
            "epoch 0, batch 21, loss 2.16764235496521\n",
            "epoch 0, batch 22, loss 2.123156785964966\n",
            "epoch 0, batch 23, loss 2.111551284790039\n",
            "epoch 0, batch 24, loss 2.1293787956237793\n",
            "epoch 0, batch 25, loss 2.0556247234344482\n",
            "epoch 0, batch 26, loss 2.032867193222046\n",
            "epoch 0, batch 27, loss 2.004666566848755\n",
            "epoch 0, batch 28, loss 1.9579436779022217\n",
            "epoch 0, batch 29, loss 1.9799003601074219\n",
            "epoch 0, batch 30, loss 1.9388948678970337\n",
            "epoch 0, batch 31, loss 1.8847649097442627\n",
            "epoch 0, batch 32, loss 1.904661774635315\n",
            "epoch 0, batch 33, loss 1.808591604232788\n",
            "epoch 0, batch 34, loss 1.8230316638946533\n",
            "epoch 0, batch 35, loss 1.7680174112319946\n",
            "epoch 0, batch 36, loss 1.7736449241638184\n",
            "epoch 0, batch 37, loss 1.7526004314422607\n",
            "epoch 0, batch 38, loss 1.6970173120498657\n",
            "epoch 0, batch 39, loss 1.655569314956665\n",
            "epoch 0, batch 40, loss 1.704801321029663\n",
            "epoch 0, batch 41, loss 1.6357901096343994\n",
            "epoch 0, batch 42, loss 1.6318962574005127\n",
            "epoch 0, batch 43, loss 1.585939884185791\n",
            "epoch 0, batch 44, loss 1.5602188110351562\n",
            "epoch 0, batch 45, loss 1.550087332725525\n",
            "epoch 0, batch 46, loss 1.5619723796844482\n",
            "epoch 0, batch 47, loss 1.5073881149291992\n",
            "epoch 0, batch 48, loss 1.5479176044464111\n",
            "epoch 0, batch 49, loss 1.571656346321106\n",
            "epoch 1, batch 0, loss 1.4990665912628174\n",
            "epoch 1, batch 1, loss 1.4141345024108887\n",
            "epoch 1, batch 2, loss 1.4357777833938599\n",
            "epoch 1, batch 3, loss 1.4282954931259155\n",
            "epoch 1, batch 4, loss 1.4458218812942505\n",
            "epoch 1, batch 5, loss 1.4225590229034424\n",
            "epoch 1, batch 6, loss 1.3269637823104858\n",
            "epoch 1, batch 7, loss 1.3546319007873535\n",
            "epoch 1, batch 8, loss 1.35769784450531\n",
            "epoch 1, batch 9, loss 1.3782942295074463\n",
            "epoch 1, batch 10, loss 1.3407491445541382\n",
            "epoch 1, batch 11, loss 1.3203932046890259\n",
            "epoch 1, batch 12, loss 1.315117359161377\n",
            "epoch 1, batch 13, loss 1.246276617050171\n",
            "epoch 1, batch 14, loss 1.2729839086532593\n",
            "epoch 1, batch 15, loss 1.3195096254348755\n",
            "epoch 1, batch 16, loss 1.2777032852172852\n",
            "epoch 1, batch 17, loss 1.2328219413757324\n",
            "epoch 1, batch 18, loss 1.2474287748336792\n",
            "epoch 1, batch 19, loss 1.266211748123169\n",
            "epoch 1, batch 20, loss 1.1932222843170166\n",
            "epoch 1, batch 21, loss 1.2169268131256104\n",
            "epoch 1, batch 22, loss 1.154868245124817\n",
            "epoch 1, batch 23, loss 1.2059272527694702\n",
            "epoch 1, batch 24, loss 1.2357733249664307\n",
            "epoch 1, batch 25, loss 1.1768414974212646\n",
            "epoch 1, batch 26, loss 1.2761863470077515\n",
            "epoch 1, batch 27, loss 1.1360723972320557\n",
            "epoch 1, batch 28, loss 1.2310587167739868\n",
            "epoch 1, batch 29, loss 1.1378544569015503\n",
            "epoch 1, batch 30, loss 1.130030632019043\n",
            "epoch 1, batch 31, loss 1.1831121444702148\n",
            "epoch 1, batch 32, loss 1.1049165725708008\n",
            "epoch 1, batch 33, loss 1.1106497049331665\n",
            "epoch 1, batch 34, loss 1.1267309188842773\n",
            "epoch 1, batch 35, loss 1.115246295928955\n",
            "epoch 1, batch 36, loss 1.086037278175354\n",
            "epoch 1, batch 37, loss 1.0949803590774536\n",
            "epoch 1, batch 38, loss 1.1465489864349365\n",
            "epoch 1, batch 39, loss 1.1080706119537354\n",
            "epoch 1, batch 40, loss 1.0635194778442383\n",
            "epoch 1, batch 41, loss 1.1194370985031128\n",
            "epoch 1, batch 42, loss 1.1240043640136719\n",
            "epoch 1, batch 43, loss 1.102461576461792\n",
            "epoch 1, batch 44, loss 1.0320898294448853\n",
            "epoch 1, batch 45, loss 1.0593782663345337\n",
            "epoch 1, batch 46, loss 1.0671613216400146\n",
            "epoch 1, batch 47, loss 1.0613036155700684\n",
            "epoch 1, batch 48, loss 1.0241533517837524\n",
            "epoch 1, batch 49, loss 1.0429117679595947\n",
            "epoch 2, batch 0, loss 1.025702953338623\n",
            "epoch 2, batch 1, loss 1.027637004852295\n",
            "epoch 2, batch 2, loss 1.0193512439727783\n",
            "epoch 2, batch 3, loss 1.0353920459747314\n",
            "epoch 2, batch 4, loss 1.0388859510421753\n",
            "epoch 2, batch 5, loss 1.0145639181137085\n",
            "epoch 2, batch 6, loss 0.9812502264976501\n",
            "epoch 2, batch 7, loss 0.9955731630325317\n",
            "epoch 2, batch 8, loss 1.0923012495040894\n",
            "epoch 2, batch 9, loss 1.0293811559677124\n",
            "epoch 2, batch 10, loss 1.0294647216796875\n",
            "epoch 2, batch 11, loss 1.022512435913086\n",
            "epoch 2, batch 12, loss 0.9699417948722839\n",
            "epoch 2, batch 13, loss 0.9758979678153992\n",
            "epoch 2, batch 14, loss 0.9969156980514526\n",
            "epoch 2, batch 15, loss 0.9939435124397278\n",
            "epoch 2, batch 16, loss 0.9891643524169922\n",
            "epoch 2, batch 17, loss 0.9903982877731323\n",
            "epoch 2, batch 18, loss 1.034375548362732\n",
            "epoch 2, batch 19, loss 1.0020259618759155\n",
            "epoch 2, batch 20, loss 0.9997695684432983\n",
            "epoch 2, batch 21, loss 0.9665695428848267\n",
            "epoch 2, batch 22, loss 0.9398335814476013\n",
            "epoch 2, batch 23, loss 0.9514763951301575\n",
            "epoch 2, batch 24, loss 1.0034892559051514\n",
            "epoch 2, batch 25, loss 0.9239028096199036\n",
            "epoch 2, batch 26, loss 0.9395431280136108\n",
            "epoch 2, batch 27, loss 0.9511045217514038\n",
            "epoch 2, batch 28, loss 0.9670810699462891\n",
            "epoch 2, batch 29, loss 0.9248679876327515\n",
            "epoch 2, batch 30, loss 0.9637289047241211\n",
            "epoch 2, batch 31, loss 0.906876802444458\n",
            "epoch 2, batch 32, loss 0.9644218683242798\n",
            "epoch 2, batch 33, loss 0.9668474197387695\n",
            "epoch 2, batch 34, loss 0.9820657968521118\n",
            "epoch 2, batch 35, loss 0.8783900737762451\n",
            "epoch 2, batch 36, loss 0.9171870946884155\n",
            "epoch 2, batch 37, loss 0.9010872840881348\n",
            "epoch 2, batch 38, loss 0.966283917427063\n",
            "epoch 2, batch 39, loss 0.9336420893669128\n",
            "epoch 2, batch 40, loss 0.8877464532852173\n",
            "epoch 2, batch 41, loss 0.9182351231575012\n",
            "epoch 2, batch 42, loss 0.9418008923530579\n",
            "epoch 2, batch 43, loss 0.9124972224235535\n",
            "epoch 2, batch 44, loss 0.9249773621559143\n",
            "epoch 2, batch 45, loss 0.9426224827766418\n",
            "epoch 2, batch 46, loss 0.8929968476295471\n",
            "epoch 2, batch 47, loss 0.8946148753166199\n",
            "epoch 2, batch 48, loss 0.8909446001052856\n",
            "epoch 2, batch 49, loss 0.9247326850891113\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(39, 64)\n",
              "    (wpe): Embedding(128, 64)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=64, out_features=39, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loss_curve = []\n",
        "optimizer = AdamW(miniGPT.parameters(), lr=5e-4)\n",
        "miniGPT.train()\n",
        "miniGPT.to(device)\n",
        "batch_size = 512\n",
        "for epoch in range(3):\n",
        "    for i in range(50):\n",
        "        batch = batch_sampler(batch_size=batch_size)\n",
        "        output = miniGPT(batch.to(device), labels=batch.to(device), )\n",
        "        loss = output.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"epoch {epoch}, batch {i}, loss {loss.item()}\")\n",
        "        loss_curve.append(loss.item())\n",
        "\n",
        "\n",
        "miniGPT.eval().to(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LK6lE5bDZSD"
      },
      "source": [
        "### Say Something? (or I'm Giving Up On You)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAXCBRvMDn0w",
        "outputId": "0ca3cb3b-1be6-4a0c-ec10-bacc4a2b3a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the dog that flew heard the short dog [End]           \n",
            "the dog that slept heard that a round horse saw that the tall bird smelled that a tall fox [End]\n",
            "the dog that flew saw a pretty bird [End]           \n",
            "the dog that sat saw a pretty short bear [End]          \n",
            "the dog that walked jumped [End]              \n"
          ]
        }
      ],
      "source": [
        "prompt = \"the dog\"\n",
        "prompt_inds = encode_sentence(prompt)\n",
        "ind_tsr = miniGPT.generate(torch.tensor(prompt_inds).long()[None, :],\n",
        "                           max_length=32, do_sample=True, top_k=0, top_p=0.9, temperature=0.7,\n",
        "                           num_return_sequences=5, pad_token_id=PAD_ID)\n",
        "for ind_seq in ind_tsr:\n",
        "    print(decode2sentence(ind_seq.tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTtBiBFYAHtA"
      },
      "source": [
        "### Does our transformer \"understand\" part-of-speech?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8ZJ0DkCAMQ4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3lbxJdOGHo5"
      },
      "source": [
        "First, let's extract the embeddings of the words and the positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOTdV7NWE7ow",
        "outputId": "a2f8a7b4-01ca-4ae5-db7b-9a65cece1f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([39, 64])\n",
            "torch.Size([128, 64])\n"
          ]
        }
      ],
      "source": [
        "token_embedding = miniGPT.transformer.wte.weight.detach()\n",
        "position_embedding = miniGPT.transformer.wpe.weight.detach()\n",
        "print(token_embedding.shape)\n",
        "print(position_embedding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW9e6bUjGNJp"
      },
      "source": [
        "We'd want to see how these learned embeddings cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s89TVRDxE34H",
        "outputId": "6f26c247-6282-43bb-fcbc-8aac372c9728"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tsne = TSNE(n_components=2, random_state=0, perplexity=15.0)\n",
        "token_embedding_2d = tsne.fit_transform(token_embedding.numpy())\n",
        "kmeans = KMeans(n_clusters=6, random_state=0).fit(token_embedding_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOSlmKNvFwSB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[10, 10])\n",
        "plt.scatter(token_embedding_2d[:, 0], token_embedding_2d[:, 1], c=kmeans.labels_)\n",
        "# annotate each word on the plot\n",
        "for i, word in enumerate(inverse_dictionary.values()):\n",
        "    plt.annotate(word, (token_embedding_2d[i, 0], token_embedding_2d[i, 1]), fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14KJx0sUHkg4"
      },
      "source": [
        "Let's print out which words are in each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2pOMTtGFByp"
      },
      "outputs": [],
      "source": [
        "nCluster = 7\n",
        "kmeans2 = KMeans(n_clusters=nCluster, random_state=42).fit(token_embedding.numpy())\n",
        "# print the list of words in each cluster\n",
        "for icluster in range(nCluster):\n",
        "    cluster_words = [word for word, ind in dictionary.items() if kmeans2.labels_[ind] == icluster]\n",
        "    print(f\"cluster {icluster}: {cluster_words}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4X0n5YjHcQ4"
      },
      "source": [
        "Overall birds eye view of the word representations (embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIeE198aGxVv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[10,5])\n",
        "plt.imshow(token_embedding)\n",
        "plt.ylabel(\"words\")\n",
        "plt.xlabel(\"embedding dim\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YxZahn9Gfc4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[5,10])\n",
        "plt.imshow(position_embedding)\n",
        "plt.ylabel(\"positions\")\n",
        "plt.xlabel(\"embedding dim\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
